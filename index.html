<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>个人主页 - Zihan Chang</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- 简单的美化 -->
  <style>
    body {
      font-family: "Helvetica Neue", Arial, sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      background: #f8f9fa;
      color: #333;
    }
    header {
      background: #2c3e50;
      color: white;
      padding: 30px 20px;
      text-align: center;
    }
    header h1 {
      margin: 0;
      font-size: 2.2em;
    }
    header p {
      margin: 5px 0;
      font-size: 1.1em;
    }
    main {
      max-width: 900px;
      margin: 20px auto;
      padding: 0 20px;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 5px;
      margin-top: 30px;
    }
    ul {
      list-style-type: none;
      padding-left: 0;
    }
    li {
      margin: 8px 0;
    }
    footer {
      text-align: center;
      padding: 15px;
      margin-top: 40px;
      background: #eee;
      font-size: 0.9em;
      color: #555;
    }
    a {
      color: #0073e6;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>

  <header>
    <h1>Zihan Chang</h1>
    <p>PhD Student | Researcher in MLsys</p>
    <p><a href="changzihan@zju.edu.cn" style="color:#f39c12;">changzihan@zju.edu.cn</a></p>
  </header>

  <main>
    <section>
      <h2>About Me</h2>
      <p>
        I am a Ph.D. student at Zhejiang University, advised by Prof. Shuibin He. Before that, 
I received my M.S. degree from Lanzhou University, supervised by Prof. Wenbo Chen. 
My research focuses on AI systems, LLM inference and training efficiency, model compression, and decentralized training & inference.
        I aim to build efficient AI systems that can be applied 
        to real-world problems.
      </p>
    </section>


    <section>
      <h2>Publications (First Author)</h2>
      <ul>
        <li><strong>[InQuant: An In-Place mixed-precision Quantization Method for KV Cache Compression with Neighbor Invasion]</strong> – Under review at AAAI, 2025. </li>
        <li><strong>[Frenzy: A Memory-Aware Serverless LLM Training System for Heterogeneous GPU Clusters]</strong> –Under review at HPCA, 2024. <a href="https://arxiv.org/html/2412.14479v1">[html]</a><a href="https://github.com/ISCS-ZJU/Frenzy">[CODE]</a></li>
        <li><strong>[SpiderFlow: Topology-Aware Scheduling for ML Training Across Decentralized GPU Clusters]</strong> – Under review at TPDS, 2023. </li>    
	<li><strong>[ Electricity price prediction based on hybrid model of adam optimized LSTM neural network and wavelet transform]</strong> – Energy (IF = 9), 2019.   <a href="https://www.sciencedirect.com/science/article/abs/pii/S0360544219314768">[HTML]</a></li>
	<li><strong>[ Effective Adam-optimized LSTM Neural Network for Electricity Price Forecasting]</strong> –, IEEE ICSESS 2018. <a href="https://ieeexplore.ieee.org/abstract/document/8663710">[HTML]</a></li>
  </ul>
    </section>

<section>
      <h2>Ongoing work</h2>
      <ul>
        <li><strong>[KV cache compression algorithm based on vLLM]</strong> – In collaboration with Sugon, 2025. 
        <li><strong>[Decentralized serving]</strong> – In collaboration with UC Merced & Amazon, 2025. <a href="https://medium.com/@yotta-labs/decentralized-inference-with-ray-and-vllm-b10f675ad239">[BLOG]</a><a href="https://github.com/ISCS-ZJU/Decentralized-inference-based-on-vLLM">[CODE]</a></li>
      </ul>
    </section>


    <section>
      <h2>Projects</h2>
      <ul>
	<li><strong> Advancing KV cache compression techniques for DeepSeek R1 based on Sugon’s domestic GPGPU BW</strong> - Project Leader,  2025.9 - .</li>
        <li><strong>The Science and Education Innovation Open Platform of the Next Generation of Artificial Intelligence</strong> – 2030 National Key Research and Development Program of China, No.2021ZD0110700, Primary contributor to Project 4, 12/2022-8/2025</li>
      </ul>
    </section>

    <section>
      <h2>Work & Intern</h2>
      <ul> 
	<li><strong>Zhejiang Lab, intern AI researcher, 2024</strong></li>
	<li><strong>Huawei AI lab, intern, 2023</strong></li>
        <li><strong>Cambricon, Deep learning high-performance computing 
library Engineer, 2020-2022 </strong></li>
        <li><strong>Reviewer, IEEE Trans on computer (TC) </strong></li>
      </ul>
    </section>
  </main>

  <footer>
    &copy; 2025 Zihan Chang | Built with GitHub Pages
  </footer>

</body>
</html>
